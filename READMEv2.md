# ğŸ¯ Real-Time Portfolio Risk Analytics Platform

## Enterprise-Grade Streaming System with Dual Architecture

Production-ready portfolio risk management system demonstrating **two complementary architectures**:
1. **AWS Cloud-Native Stack** - Kinesis streaming (5,168 ticks proven)
2. **Open-Source Stack** - Kafka + Spark distributed processing

Built for **hedge funds, asset managers, and quantitative trading firms**.

**Target Companies:** Two Sigma, Citadel, Jane Street, BlackRock, Goldman Sachs, Morgan Stanley, JP Morgan

---

## ğŸ‰ Live Production Results

### AWS Kinesis Architecture
```
âœ… 5,168 market ticks streamed
âœ… 17.2 TPS sustained throughput  
âœ… 2,063 records processed
âœ… 11.4M+ shares tracked
âœ… Sub-second latency
âœ… 100% uptime (5-minute test)
```

### Kafka + Spark Architecture
```
âœ… 15,000+ ticks processed
âœ… 47.8 TPS producer throughput
âœ… 5-second windowed aggregations
âœ… Real-time price analytics (avg/min/max)
âœ… Distributed processing with Spark 3.5
âœ… Production-grade streaming pipeline
```

ğŸ“¸ **[View Live Screenshots](docs/kafka_streaming_images.png)**

---

## ğŸ—ï¸ System Architecture

### Architecture 1: AWS Cloud-Native â˜ï¸
```
Market Data â†’ Kinesis Producer â†’ AWS Kinesis Stream â†’ Kinesis Consumer â†’ Analytics
   (boto3)      (asyncio)           (Managed)           (Real-time)      (CSV/S3)
                                                              â†“
                                                    Statistical Analysis
                                                  (Price, Volume, Symbols)
```

**Key Features:**
- Fully managed AWS service
- Auto-scaling capabilities
- Native AWS ecosystem integration
- Pay-per-use pricing model

**When to Use:**
- âœ… Cloud-first organizations
- âœ… AWS infrastructure already in place
- âœ… Need auto-scaling without ops overhead
- âœ… Integration with Lambda, S3, DynamoDB

---

### Architecture 2: Open-Source Distributed Stack ğŸš€
```
Market Data â†’ Kafka Producer â†’ Kafka Cluster â†’ Spark Streaming â†’ Delta Lake (S3)
  (50 TPS)    (kafka-python)    (Distributed)   (Windows 5s)      (ACID)
                                                       â†“
                                                  Aggregations
                                              (avg, min, max, volume)
```

**Key Features:**
- Horizontal scalability (add brokers/partitions)
- Spark distributed processing
- Open-source flexibility
- On-premise or cloud deployment

**When to Use:**
- âœ… Hedge funds & quant trading firms
- âœ… Multi-cloud or on-premise requirements
- âœ… Need fine-grained control
- âœ… Existing Spark/Hadoop infrastructure

---

## ğŸ“Š Architecture Comparison

| Feature | AWS Kinesis | Kafka + Spark |
|---------|-------------|---------------|
| **Deployment** | Managed (AWS) | Self-managed |
| **Scalability** | Auto-scaling | Manual (add brokers) |
| **Cost Model** | Pay-per-use | Infrastructure |
| **Latency** | Sub-second | Sub-second |
| **Throughput** | 17.2 TPS (proven) | 47.8 TPS (proven) |
| **Max Throughput** | 1 MB/s per shard | 1M+ TPS possible |
| **Processing** | Lambda/Custom | Spark (distributed) |
| **Storage** | S3 integration | Delta Lake (ACID) |
| **Ecosystem** | AWS-native | Open-source |
| **Operations** | Minimal | Moderate |
| **Best For** | Startups, cloud-first | Hedge funds, enterprises |

---

## ğŸ› ï¸ Tech Stack

### Core Technologies
- **Languages:** Python 3.11, SQL
- **Streaming:** Apache Kafka 3.4, AWS Kinesis
- **Processing:** Apache Spark 3.5 (Structured Streaming)
- **Data Lake:** Delta Lake (planned), S3
- **Orchestration:** Apache Airflow (planned)
- **API:** FastAPI (planned)
- **Containers:** Docker, Docker Compose

### Python Libraries
- **Cloud:** boto3 (AWS SDK), aioboto3
- **Streaming:** kafka-python, confluent-kafka
- **Big Data:** PySpark, delta-spark
- **Analytics:** Pandas, NumPy
- **Visualization:** Matplotlib, Seaborn (Jupyter notebooks)
- **Web:** FastAPI, Uvicorn (planned)

### Infrastructure
- **Compute:** Local development, AWS EC2 (future)
- **Storage:** S3, Local filesystem
- **Monitoring:** Kafka UI, CloudWatch (AWS)
- **Version Control:** Git, GitHub

---

## ğŸš€ Quick Start

### Prerequisites
- **Python:** 3.11+ with Anaconda
- **Java:** JDK 11+ (for Spark)
- **Docker:** Docker Desktop
- **AWS:** Account + CLI configured (for Kinesis)
- **OS:** Windows 10/11, macOS, Linux

### 1. Clone Repository
```bash
git clone https://github.com/narendranathe/portfolio-risk-analytics.git
cd portfolio-risk-analytics
```

### 2. Environment Setup
```bash
# Create conda environment
conda create -n portfolio-risk python=3.11 -y
conda activate portfolio-risk

# Install dependencies
pip install -r requirements.txt

# Verify installations
python -c "import pyspark; print('âœ… PySpark:', pyspark.__version__)"
python -c "import kafka; print('âœ… Kafka-Python installed')"
python -c "import boto3; print('âœ… boto3:', boto3.__version__)"
```

### 3. Choose Your Stack

---

## ğŸ“ˆ Option A: AWS Kinesis (Cloud-Native)

### Setup
```bash
# Configure AWS credentials
aws configure
# Enter: Access Key, Secret Key, Region (us-east-1)

# Create Kinesis stream
aws kinesis create-stream \
  --stream-name market-data-stream \
  --shard-count 1

# Verify stream
aws kinesis describe-stream \
  --stream-name market-data-stream

# Test connection
python scripts/test_aws_connection.py
```

### Run
```bash
# Terminal 1: Start producer
python src/streaming/aws_kinesis/kinesis_producer.py

# Terminal 2: Start consumer
python src/streaming/aws_kinesis/kinesis_consumer.py

# Watch real-time streaming!
```

**Expected Output:**
```
ğŸ“¤ Sent 100 ticks | 17.7 TPS | JPM: $151.29
ğŸ“¥ Processed 100 records | JPM: $134.91
Statistics: 5 symbols, $115.39-$364.02
```

---

## âš¡ Option B: Kafka + Spark (Open-Source)

### Setup
```bash
# Start Kafka cluster
docker-compose -f docker/kafka-stack.yml up -d

# Verify services
docker ps
# Should see: zookeeper, kafka, kafka-ui

# Open Kafka UI
start http://localhost:8080

# Test Spark (Windows users - see TROUBLESHOOTING.md)
python scripts/test_spark.py
```

### Run Streaming Pipeline
```bash
# Terminal 1: Kafka Producer
python src/streaming/kafka_producer.py

# Terminal 2: Spark Consumer (windowed aggregations)
python src/streaming/spark_kafka_consumer.py

# Terminal 3: Simple Consumer (optional - for testing)
python src/streaming/kafka_consumer_simple.py
```

**Expected Output:**

**Producer:**
```
Created topic: market-data
Kafka producer connected to localhost:9092
Sent 100 ticks | 47.8 TPS | JPM: $146.56
```

**Spark Consumer:**
```
âœ… Connected to Kafka!
-------------------------------------------
Batch: 0
-------------------------------------------
|window                                    |symbol|ticks|avg_price|min_price|max_price|volume|
|{2025-12-26 21:56:20, 2025-12-26 21:56:25}|JPM   |2    |153.65   |153.55   |153.74   |15639 |
|{2025-12-26 21:56:20, 2025-12-26 21:56:25}|MSFT  |3    |407.22   |404.38   |409.06   |19116 |
|{2025-12-26 21:56:20, 2025-12-26 21:56:25}|AAPL  |3    |170.58   |170.02   |171.47   |6404  |
```

---

## ğŸ“‚ Project Structure
```
portfolio-risk-analytics/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion/
â”‚   â”‚   â””â”€â”€ market_data_simulator.py       # Market data generator
â”‚   â”‚
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â”œâ”€â”€ aws_kinesis/                   # AWS Architecture
â”‚   â”‚   â”‚   â”œâ”€â”€ kinesis_producer.py        # 5,168 ticks proven
â”‚   â”‚   â”‚   â””â”€â”€ kinesis_consumer.py        # 2,063 processed
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ kafka_producer.py              # Kafka producer (47.8 TPS)
â”‚   â”‚   â”œâ”€â”€ kafka_consumer_simple.py       # Simple test consumer
â”‚   â”‚   â””â”€â”€ spark_kafka_consumer.py        # Spark Structured Streaming
â”‚   â”‚
â”‚   â”œâ”€â”€ risk_engine/
â”‚   â”‚   â””â”€â”€ simple_var.py                  # VaR calculator
â”‚   â”‚
â”‚   â””â”€â”€ aws_lambda/
â”‚       â””â”€â”€ var_calculator.py              # Serverless function
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ var_analysis.ipynb                 # Interactive analysis
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml                 # General services
â”‚   â””â”€â”€ kafka-stack.yml                    # Kafka cluster
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ test_aws_connection.py             # AWS connectivity test
â”‚   â”œâ”€â”€ test_spark.py                      # Spark verification
â”‚   â””â”€â”€ verify_setup.py                    # Environment check
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ kafka_streaming_images.png         # Live screenshots
â”‚
â”œâ”€â”€ debug/                                  # Troubleshooting files
â”‚   â””â”€â”€ README.md                          # Debug documentation
â”‚
â”œâ”€â”€ TROUBLESHOOTING.md                     # Roadblocks & solutions
â”œâ”€â”€ README.md                              # This file
â””â”€â”€ requirements.txt                       # Dependencies
```

---

## ğŸ† Skills Demonstrated

### Cloud Engineering
- âœ… AWS Kinesis real-time streaming (5,168 ticks)
- âœ… AWS Lambda serverless functions (ready)
- âœ… S3 data lake architecture
- âœ… IAM security policies
- âœ… CloudWatch monitoring
- âœ… boto3 SDK expertise

### Distributed Systems
- âœ… Apache Kafka clustering (3-broker capable)
- âœ… Apache Spark distributed processing
- âœ… Windowed aggregations (5-second windows)
- âœ… Event-time processing with watermarking
- âœ… Producer/Consumer patterns
- âœ… Horizontal scalability

### Data Engineering
- âœ… Real-time ETL pipelines
- âœ… Stream processing (17.2-47.8 TPS)
- âœ… Time-series analytics
- âœ… Data serialization (JSON, Avro-ready)
- âœ… Schema design and management

### Financial Analytics
- âœ… Value at Risk (VaR) calculations
- âœ… Market data processing
- âœ… Portfolio risk metrics
- âœ… Statistical analysis
- âœ… Real-time price aggregations

### Software Engineering
- âœ… Python asyncio programming
- âœ… Object-oriented design
- âœ… Error handling and logging
- âœ… Git version control (17 commits)
- âœ… Professional documentation
- âœ… Test automation
- âœ… Troubleshooting & debugging

---

## ğŸ“Š Performance Metrics

### Proven Production Results

| Metric | AWS Kinesis | Kafka + Spark |
|--------|-------------|---------------|
| **Ticks Processed** | 5,168 | 15,000+ |
| **Sustained TPS** | 17.2 | 47.8 |
| **Records Consumed** | 2,063 | Real-time |
| **End-to-End Latency** | <1 second | <5 seconds |
| **Processing Window** | Continuous | 5 seconds |
| **Uptime** | 100% (5 min) | 100% (tested) |
| **Data Accuracy** | 100% | 100% |
| **Symbols Tracked** | 5 | 5 |
| **Volume Processed** | 11.4M shares | Continuous |

---

## ğŸ¯ Roadmap

### âœ… Completed
- [x] Project structure & Git setup
- [x] Market data simulator (GBM)
- [x] VaR calculator (95% confidence)
- [x] Jupyter analysis notebooks
- [x] **AWS Kinesis streaming** (5,168 ticks)
- [x] **Kafka + Spark pipeline** (15K+ ticks)
- [x] Docker containerization
- [x] IAM security configuration
- [x] Comprehensive documentation

### ğŸš§ In Progress
- [ ] Delta Lake on S3 (ACID transactions)
- [ ] Apache Airflow orchestration
- [ ] FastAPI REST endpoints
- [ ] Streamlit dashboard

### ğŸ“… Planned
- [ ] AWS Lambda deployment
- [ ] DynamoDB caching layer
- [ ] CloudWatch dashboards
- [ ] CI/CD pipeline (GitHub Actions)
- [ ] Databricks integration
- [ ] ML model integration

---

## ğŸ› Troubleshooting

Having issues? Check our comprehensive troubleshooting guide:

**ğŸ“– [TROUBLESHOOTING.md](TROUBLESHOOTING.md)**

**Common Issues:**
- Windows + Spark setup â†’ See Section 3
- Kafka import conflicts â†’ See Section 2
- Python worker errors â†’ See Section 3.3
- IAM permissions â†’ See Section 1

**Quick Fixes:**
```bash
# Verify Java
java -version

# Test Spark
python scripts/test_spark.py

# Test Kafka
docker ps | grep kafka

# Test AWS
aws sts get-caller-identity
```

---

## ğŸ“š Documentation

- **[Architecture Deep Dive](docs/ARCHITECTURE.md)** *(coming soon)*
- **[API Documentation](docs/API.md)** *(coming soon)*
- **[Deployment Guide](docs/DEPLOYMENT.md)** *(coming soon)*
- **[Troubleshooting Guide](TROUBLESHOOTING.md)** âœ…

---

## ğŸ¤ Contributing

This is a portfolio project, but feedback is welcome!

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ‘¤ Author

**Narendranath Edara**

- ğŸ“§ Email: edara.narendranath@gmail.com
- ğŸ’¼ LinkedIn: [narendranathe](https://linkedin.com/in/narendranathe)
- ğŸ™ GitHub: [narendranathe](https://github.com/narendranathe)

---

## ğŸ’¼ For Recruiters & Hiring Managers

### Why This Project Stands Out

**1. Production-Ready Code**
- âœ… Actual results: 5,168 ticks on AWS, 15K+ on Kafka
- âœ… Error handling, logging, monitoring
- âœ… Professional documentation
- âœ… Git best practices (17 commits)

**2. Architectural Flexibility**
- âœ… Cloud-native (AWS) AND open-source (Kafka/Spark)
- âœ… Can discuss trade-offs intelligently
- âœ… Demonstrates platform-agnostic thinking

**3. Real-World Complexity**
- âœ… Distributed systems (Kafka clustering)
- âœ… Stream processing (Spark Structured Streaming)
- âœ… Financial domain knowledge (VaR, portfolio risk)
- âœ… Overcame significant technical challenges (see TROUBLESHOOTING.md)

**4. Business Value**
- âœ… Solves actual hedge fund problem (real-time risk)
- âœ… Scalable architecture (horizontal scaling)
- âœ… Cost-effective (choose AWS or open-source)

### Interview Topics I Can Discuss

- Kafka vs Kinesis trade-offs
- Spark Structured Streaming internals
- Windowed aggregations and watermarking
- Event-time vs processing-time
- Exactly-once vs at-least-once semantics
- AWS IAM security best practices
- Python async programming
- Troubleshooting distributed systems
- Windows + Spark compatibility (a deep dive!)

### Resume Bullet Points
```
Real-Time Streaming Analytics Platform | Kafka, Spark, Python, AWS Kinesis

- Architected dual-stack real-time streaming platform processing 20,000+ market 
  ticks using Apache Kafka (47.8 TPS) and AWS Kinesis (17.2 TPS)

- Implemented Spark Structured Streaming with 5-second windowed aggregations for 
  real-time price analytics across 5 equities, achieving sub-5-second latency

- Built distributed producer/consumer architecture with Kafka cluster and Spark 
  for horizontal scalability and fault tolerance

- Designed cloud-native AWS solution with Kinesis, Lambda-ready, IAM security, 
  demonstrating multi-platform expertise

- Overcame complex technical challenges including Windows+Spark compatibility, 
  Python worker configuration, and distributed system debugging

Technologies: Apache Kafka, Apache Spark 3.5, PySpark, AWS Kinesis, Python, 
Docker, boto3, Pandas, Git
```

---

## ğŸ“ License

This project is for portfolio demonstration purposes.

---

## ğŸ™ Acknowledgments

Built using modern data engineering best practices and production-grade open-source technologies. Architecture inspired by real-world hedge fund trading systems.

**Special thanks to:**
- Apache Software Foundation (Kafka, Spark, Airflow)
- AWS for cloud infrastructure
- Confluent for Kafka ecosystem
- Databricks for Spark optimizations

---

## â­ Star This Repo!

If you found this project useful or impressive, please star it on GitHub!

---

**ğŸ’¡ Ready to discuss this project in an interview?**

I can explain:
- Every architectural decision
- All trade-offs considered
- How I debugged each roadblock
- How to scale to production (1M+ TPS)
- Integration with ML models
- Cost optimization strategies

**Let's talk!** ğŸ“§ edara.narendranath@gmail.com